
````markdown
# LiP-LLM: Large Language Models for Logical Plan Generation and Multi-Robot Task Allocation

## Project Overview

This project implements the core concepts of the "LiP-LLM" paper, demonstrating how Large Language Models (LLMs) can be leveraged to generate logical task plans and efficiently allocate them to a team of multiple robots. The system takes a natural language instruction from a user, decomposes it into a sequence of robot-executable skills, builds a dependency graph, and then uses linear programming to assign tasks to available robots for parallel execution.

---

## Visual Demonstration

The image below illustrates the high-level process:
![LiP-LLM Overview](assets\plot.png)
*(Note: The `visual_demonstration.png` image should be replaced with the actual image you generated and hosted, or placed in an `images` folder within your repo and linked accordingly.)*

---

## Core Modules

The project is structured into three main Python modules, orchestrated by a central script:

1.  **Skill List Generation (`src/skill_generator.py`)**:
    * **Input**: Natural language instruction (e.g., "Stack the red block on the blue block").
    * **Process**: Uses a Gemini LLM (e.g., `gemini-1.5-flash-latest`) with carefully crafted few-shot prompts to decompose the instruction into a precise list of robot-executable skills (e.g., `['pick_and_place(blue block, table)', 'pick_and_place(red block, blue block)']`). This module emphasizes avoiding redundant steps and ensures all generated skills are from a predefined set.

2.  **Dependency Graph Generation (`src/graph_generator.py`)**:
    * **Input**: The generated skill list.
    * **Process**: Queries the Gemini LLM to identify temporal dependencies between skills (e.g., placing `red block` on `blue block` depends on `blue block` being in place). It constructs a Directed Acyclic Graph (DAG) using the `networkx` library, representing these prerequisites. Includes logic to detect and resolve cycles.

3.  **Task Allocation & Execution (`src/task_allocator.py`)**:
    * **Input**: The dependency graph and a list of available robots with their capabilities.
    * **Process**: Continuously identifies all currently executable (root) skills from the graph. It calculates a "weight" for assigning each skill to each robot based on factors like distance and capability. A Mixed-Integer Linear Programming (MILP) solver (`scipy.optimize.milp`) then finds the optimal assignment of skills to robots for the current timestep. Once skills are "completed," they are removed from the graph, making new skills executable in subsequent steps.

---

## Setup and Installation

### 1. Clone the Repository

```bash
git clone [https://github.com/your-username/lip-llm-project.git](https://github.com/your-username/lip-llm-project.git) # Replace with your repo URL
cd lip-llm-project
````

### 2\. Create and Activate a Virtual Environment

```bash
python -m venv venv
# On Windows:
.\venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate
```

### 3\. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4\. Configure Gemini API Key

1.  Obtain a Gemini API key from [Google AI Studio](https://aistudio.google.com/).
2.  Create a `.env` file in the root of your project:
    ```
    # .env
    GEMINI_API_KEY="YOUR_GEMINI_API_KEY_HERE"
    ```

### 5\. Asset Files (for 3D Visualization)

For the PyBullet 3D visualization, ensure you have the following folder structure within your project root, populated with the respective URDF/OBJ files:

```
/your_project_folder/
|-- assets/
    |-- bowl/
        |-- bowl/
            |-- cup.obj
            |-- bowl.urdf # (or other bowl-related files)
    |-- robotiq_2f_85/
        |-- robotiq_2f_85/
            |-- robotiq_2f_85.urdf
            |-- textures/ # (and other gripper files)
    |-- ur5e/
        |-- ur5e/
            |-- ur5e.urdf
            |-- collision/ # (and other UR5e files)
            |-- visual/
```

*Note: Standard URDF files for a table and simple colored blocks are either loaded from PyBullet's data path or procedurally generated by the visualization script.*

-----

## How to Run

### 1\. Command Line Execution

You can run the full planning and execution simulation from your terminal:

```bash
python main.py
```

### 2\. Jupyter Notebook (Recommended for Interactive Exploration)

For an interactive experience including detailed output and 3D visualizations, open and run the provided Jupyter Notebook:

```bash
jupyter notebook lipllm.ipynb # Assuming your notebook is named lipllm.ipynb
```

Follow the cells sequentially, pasting your API key in Cell 1.

-----

## Project Structure

```
lip_llm_project/
├── .env                        # Environment variables (API Key)
├── main.py                     # Main script to run the full pipeline
├── requirements.txt            # Python dependencies
├── lipllm.ipynb                # Jupyter Notebook for interactive development and visualization
└── src/
    ├── __init__.py             # Makes 'src' a Python package
    ├── skill_generator.py      # Module for generating skill lists from NL
    ├── graph_generator.py      # Module for building skill dependency graphs
    └── task_allocator.py       # Module for allocating tasks to robots using MILP
└── assets/                     # 3D model files for PyBullet visualization
    ├── bowl/
    ├── robotiq_2f_85/
    └── ur5e/
```

-----

## Future Enhancements

  * **Inverse Kinematics (IK)**: Implement realistic robot arm movement for pick-and-place actions instead of instant teleportation.
  * **Collision Avoidance**: Integrate collision detection and path planning into the task execution.
  * **Dynamic Environment**: Allow for real-time changes in the environment or robot status.
  * **More Complex Skills**: Expand the predefined skill set to include more advanced robot manipulations.
  * **Error Handling**: Robust error recovery mechanisms for failed tasks or LLM responses.

-----

## Contributing

Feel free to fork this repository, open issues, and submit pull requests. Any contributions are welcome\!

-----

## License

This project is open-source. Please refer to the `LICENSE` file for more details. (You might want to add a `LICENSE` file, e.g., MIT License).

```

---

You now have a complete `README.md` that explains your project clearly, provides setup instructions, and highlights its key features\!

One final detail: If you want the image link in the `README.md` to work, you'll need to upload the generated image (the one I created earlier) to a place where it's publicly accessible (like GitHub's image hosting in your repo, or an image hosting service) and then update the URL in the `README.md` file.
```